# ğŸ•µï¸ Fraud Detection on IEEE-CIS Dataset

## ğŸ“Œ Overview
This project implements a **Fraud Detection System** using the [IEEE-CIS Fraud Detection dataset](https://www.kaggle.com/c/ieee-fraud-detection).  
The dataset is highly imbalanced and complex, simulating real-world online transactions.  
The goal is to build **machine learning pipelines** that can detect fraudulent transactions effectively while minimizing false positives.  

---

## âš™ï¸ Project Structure
fraud-detection-ieee/
â”‚
â”œâ”€â”€ src/ # Source code
â”‚ â”œâ”€â”€ eda.py # Exploratory Data Analysis
â”‚ â”œâ”€â”€ preprocess.py # Data preprocessing & feature engineering
â”‚ â”œâ”€â”€ train.py # Model training script
â”‚ â”œâ”€â”€ evaluate.py # Evaluation metrics & results
â”‚ â””â”€â”€ utils.py # Helper functions
â”‚
â”œâ”€â”€ reports/ # Visualizations & generated plots
â”‚
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ .gitignore # Ignored files (datasets, cache, etc.)
â””â”€â”€ README.md # Project documentation

yaml
Copy
Edit

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Ciyakj/fraud-detection-ieee.git
cd fraud-detection-ieee
2ï¸âƒ£ Create and activate a virtual environment
bash
Copy
Edit
python -m venv venv
venv\Scripts\activate   # On Windows
source venv/bin/activate  # On Mac/Linux
3ï¸âƒ£ Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
ğŸ“Š Usage
ğŸ§¹ Run preprocessing
bash
Copy
Edit
python src/preprocess.py
ğŸ” Run Exploratory Data Analysis (EDA)
bash
Copy
Edit
python src/eda.py
ğŸ¤– Train the model
bash
Copy
Edit
python src/train.py
ğŸ“ˆ Evaluate model performance
bash
Copy
Edit
python src/evaluate.py
ğŸ“‚ Data
The dataset comes from the IEEE-CIS Fraud Detection Kaggle competition.

Due to size restrictions, raw data is not included in this repo.

Download it from Kaggle: IEEE-CIS Dataset.

Place the extracted files inside a data/raw/ folder.

ğŸ“Š Results (Sample)
Models tested: Logistic Regression, Random Forest, XGBoost, LightGBM

Metrics used: AUC, F1-Score, Precision, Recall

Example output (on test split):

AUC: 0.91

F1-Score: 0.84

Precision: 0.80

Recall: 0.88

ğŸ› ï¸ Tech Stack
Python 3.11

Pandas, NumPy, Scikit-learn

Matplotlib, Seaborn

XGBoost, LightGBM

ğŸ“Œ Next Steps
Add deep learning models (LSTMs, Autoencoders)

Perform hyperparameter tuning

Implement model explainability (SHAP, LIME)

Deploy a Flask/Streamlit app for live fraud detection demo

ğŸ‘©â€ğŸ’» Author
Ciya K J
MSc Data Science | Aspiring Data Scientist | Passionate about ML & AI

ğŸ”— LinkedIn | GitHub
